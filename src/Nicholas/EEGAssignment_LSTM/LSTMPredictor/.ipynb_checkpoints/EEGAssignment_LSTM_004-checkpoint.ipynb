{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "testID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# SkLearn\n",
    "import sklearn.preprocessing as SklPreProcessing\n",
    "import sklearn.metrics as SklMetrics\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoring_Functions():\n",
    "    #\n",
    "    def __init__(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "    #\n",
    "    def accuracy(self):\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "        return str(accuracy_score(self.y_true, self.y_pred) * 100) + \"%\"\n",
    "    #\n",
    "    def precision(self):\n",
    "        # http: // scikit - learn.org / stable / modules / generated / sklearn.metrics.precision_score.html  # sklearn.metrics.precision_score\n",
    "        return str(precision_score(self.y_true, self.y_pred, average='weighted')* 100) + '%'\n",
    "    #\n",
    "    def recall(self):\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "        return str(recall_score(self.y_true, self.y_pred, average='weighted') * 100) + '%'\n",
    "    #\n",
    "    def f_measure(self):\n",
    "        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "        return str(f1_score(self.y_true, self.y_pred, average='weighted') * 100) + '%'\n",
    "    #\n",
    "    def scoring_results(self):\n",
    "        return \"Accuracy: \" + str(self.accuracy()) + \"\\nPrecision: \" + str(self.precision()) + \"\\nRecall: \" + str(self.recall()) + \"\\nFMeasure: \" + str(self.f_measure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor:\n",
    "    ############################\n",
    "    # MetaData\n",
    "    __testID = None\n",
    "    __cmdCount = 0\n",
    "    __cmdHistory = ''\n",
    "    __resultsRootDir = 'results'\n",
    "    ############################\n",
    "    # Dataset info\n",
    "    __eegData = None\n",
    "    __eegHeader = None\n",
    "    __dataFileName = None\n",
    "    ############################\n",
    "    # ML model info\n",
    "    __model = None\n",
    "    __trainTestMap = None\n",
    "    ############################\n",
    "   \n",
    "    def __init__(self, __testID, filename):\n",
    "        self.__testID = str(__testID).zfill(5)\n",
    "        self.__createTestResultsDir()\n",
    "        self.__dataFileName = filename\n",
    "        self.ResetState()\n",
    "        \n",
    "    def ResetState(self, filename=None):\n",
    "        tmpFileName = ''\n",
    "        self.__cmdHistory = ''\n",
    "        self.__cmdCount = 0\n",
    "        \n",
    "        if filename is None:\n",
    "            tmpFileName = self.__dataFileName\n",
    "        else:\n",
    "            tmpFileName = filename\n",
    "            \n",
    "        self.__loadData(tmpFileName)\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Plot methods\n",
    "    def PlotFeatures(self, filePrefix=None, figWidth=20, figHeight=20):\n",
    "        self.__addToHistory('PlotFeatures_filePrefix:' + str(filePrefix))\n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "        \n",
    "        featureCount = len(self.__eegHeader) - 1\n",
    "        idxSubplot = 0\n",
    "        \n",
    "        for itr in range(featureCount):\n",
    "            idxSubplot = idxSubplot + 1\n",
    "            currAx = fig.add_subplot(featureCount + 1, 1, idxSubplot)\n",
    "            currAx.grid()\n",
    "            currAx.plot(self.__eegData[:, itr])\n",
    "            currAx.set_title(self.__eegHeader[itr])\n",
    "        \n",
    "        idxSubplot = idxSubplot + 1\n",
    "        currAx = fig.add_subplot(featureCount + 1, 1, idxSubplot)\n",
    "        currAx.grid()\n",
    "        currAx.plot(self.__eegData[:, -1])\n",
    "        currAx.set_title(self.__eegHeader[-1])\n",
    "\n",
    "        if filePrefix is None:\n",
    "            plt.show()\n",
    "        else:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + filePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "            \n",
    "    def PlotBoxPlots(self, filePrefix=None, figWidth=20, figHeight=20):\n",
    "        self.__addToHistory('PlotBoxPlots_filePrefix:' + str(filePrefix))\n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "        \n",
    "        ax1 = fig.add_subplot(2, 1, 1)\n",
    "        ax1.set_title('Feature range: Full')\n",
    "        ax1.boxplot(self.__eegData[:, :-1]\n",
    "                    , sym='b.'\n",
    "                    , vert=False\n",
    "                    , whis='range'\n",
    "                    , labels=self.__eegHeader[:-1]\n",
    "                    , meanline=True\n",
    "                    , showbox=True\n",
    "                    , showfliers=True)\n",
    "        \n",
    "        ax2 = fig.add_subplot(2, 1, 2)\n",
    "        ax2.set_title('Feature range: [5%, 95%]')\n",
    "        ax2.boxplot(self.__eegData[:, :-1]\n",
    "                    , sym='b.'\n",
    "                    , vert=False\n",
    "                    , whis=[5, 95]\n",
    "                    , labels=self.__eegHeader[:-1]\n",
    "                    , meanline=True\n",
    "                    , showbox=True\n",
    "                    , showfliers=False)\n",
    "        \n",
    "        if filePrefix is not None:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + filePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "            \n",
    "    def PlotMeanDistribution(self, filePrefix=None, figWidth=20, figHeight=20):\n",
    "        self.__addToHistory('PlotMeanDistribution_filePrefix:' + str(filePrefix))\n",
    "        idxEyeClosed = np.where(self.__eegData[:, -1] == 1)[0]\n",
    "        idxEyeOpened = np.where(self.__eegData[:, -1] == 0)[0]\n",
    "        \n",
    "        featureCount = len(self.__eegHeader) - 1\n",
    "        xTick = np.array(range(featureCount))\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.grid()\n",
    "        ax.set_title('Mean Distribution')\n",
    "        ax.set_xticklabels(self.__eegHeader[0:-1])\n",
    "        ax.set_xticks(xTick)\n",
    "\n",
    "        ax.plot(xTick, self.__eegData[idxEyeOpened][:, 0:featureCount].mean(axis=0), 'bo')\n",
    "        ax.plot(xTick, self.__eegData[idxEyeClosed][:, 0:featureCount].mean(axis=0), 'ro')\n",
    "        ax.plot(xTick, self.__eegData[:, 0:featureCount].mean(axis=0), 'go')\n",
    "\n",
    "        ax.legend(['Eye Open', 'Eye Closed', 'Both'])\n",
    "        \n",
    "        if filePrefix is not None:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + filePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "\n",
    "    def PlotStdDevDistribution(self, filePrefix=None, figWidth=20, figHeight=20):\n",
    "        self.__addToHistory('PlotStdDevDistribution_filePrefix:' + str(filePrefix))\n",
    "        idxEyeClosed = np.where(self.__eegData[:, -1] == 1)[0]\n",
    "        idxEyeOpened = np.where(self.__eegData[:, -1] == 0)[0]\n",
    "        \n",
    "        featureCount = len(self.__eegHeader) - 1\n",
    "        xTick = np.array(range(featureCount))\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.grid()\n",
    "        ax.set_title('Standard Deviation Distribution')\n",
    "        ax.set_xticklabels(self.__eegHeader[0:-1])\n",
    "        ax.set_xticks(xTick)\n",
    "\n",
    "        ax.plot(xTick, self.__eegData[idxEyeOpened][:, 0:featureCount].std(axis=0), 'bo')\n",
    "        ax.plot(xTick, self.__eegData[idxEyeClosed][:, 0:featureCount].std(axis=0), 'ro')\n",
    "        ax.plot(xTick, self.__eegData[:, 0:featureCount].std(axis=0), 'go')\n",
    "\n",
    "        ax.legend(['Eye Open', 'Eye Closed', 'Both'])\n",
    "        \n",
    "        if filePrefix is not None:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + filePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "            \n",
    "    def FeatureCorrelationMatrix(self, filePrefix=None, figWidth=20, figHeight=20):\n",
    "        self.__addToHistory('FeatureCorrelationMatrix_filePrefix:' + str(filePrefix))\n",
    "        # Compute the correlation matrix\n",
    "        corr = pd.DataFrame(data=self.__eegData[:, :-1], columns=list(self.__eegHeader[:-1])).corr()\n",
    "        \n",
    "        # Generate a mask for the upper triangle\n",
    "        mask = np.zeros_like(corr, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        \n",
    "        # Set up the matplotlib figure\n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "        \n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.set_title(\"Feature correlation matrix\")\n",
    "        \n",
    "        # Generate a custom diverging colormap\n",
    "        cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "        \n",
    "        # Draw the heatmap\n",
    "        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
    "                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "        \n",
    "        if filePrefix is not None:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + filePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Getters\n",
    "    def GetFeatureNames(self):\n",
    "        return self.__eegHeader[:-1]\n",
    "    \n",
    "    def GetCmdHistory(self):\n",
    "        return self.__cmdHistory\n",
    "    ###########################################################################\n",
    "    # Dataset operations\n",
    "    def RemoveFeatures(self, indices):\n",
    "        assert isinstance(indices, list), 'Parameter must be a list'\n",
    "\n",
    "        self.__addToHistory('RemoveFeatures:' + ','.join(str(x) for x in indices))\n",
    "        self.__eegHeader = np.delete(self.__eegHeader, indices)\n",
    "        self.__eegData = np.delete(self.__eegData, indices, axis=1)\n",
    "    \n",
    "    def RemoveOutliers(self, upperLimit=None, lowerLimit=None):\n",
    "        outLierIndexes = set()\n",
    "        result = {}\n",
    "        result['upper'] = set()\n",
    "        result['lower'] = set()\n",
    "        \n",
    "        self.__addToHistory('RemoveOutliers_Upper:' + str(upperLimit)\n",
    "                            + '_Lower:' + str(lowerLimit))\n",
    "        featureCount = len(self.__eegHeader) - 1\n",
    "\n",
    "        if upperLimit is not None:\n",
    "            for itr in range(featureCount):\n",
    "                outLiers = np.where(self.__eegData[:, itr] > upperLimit)[0]\n",
    "                if len(outLiers) > 0:\n",
    "                    for i in xrange(len(outLiers)):\n",
    "                        result['upper'].add(outLiers[i])\n",
    "\n",
    "        if lowerLimit is not None:\n",
    "            for itr in range(featureCount):\n",
    "                outLiers = np.where(self.__eegData[:, itr] < lowerLimit)[0]\n",
    "                if len(outLiers) > 0:\n",
    "                    for i in xrange(len(outLiers)):\n",
    "                        result['lower'].add(outLiers[i])\n",
    "                        \n",
    "        deleteIndexes = list(result['upper'].union(result['lower']))\n",
    "        self.__eegData = np.delete(self.__eegData, deleteIndexes, axis=0)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def NormalizeData(self, normFunc='MinMax'):\n",
    "        self.__addToHistory('NormalizeData_Func:' + normFunc)\n",
    "        \n",
    "        if normFunc == 'MinMax':\n",
    "            scaler = SklPreProcessing.MinMaxScaler(feature_range=(0, 1))\n",
    "            self.__eegData = scaler.fit_transform(self.__eegData)\n",
    "        else:\n",
    "            self.__eegData = normalize(self.__eegData, norm=normFunc)\n",
    "        \n",
    "    ###########################################################################\n",
    "    # Keras LSTM model\n",
    "    def CompileModel(self, lagCount, testSetRatio, neuronCount, dropOut,\n",
    "                     useLaggedOutput, modelArchitectureFilename,\n",
    "                     lossFunc='mae', optimizerFunc='adam'):\n",
    "        self.__addToHistory('CompileModel_Lag:' + str(lagCount)\n",
    "                            + '_TestSize:' + str(testSetRatio)\n",
    "                            + '_NeuronCount:' + str(neuronCount)\n",
    "                            + '_DropOut:' + str(dropOut)\n",
    "                            + '_useLaggedOutput:' + str(useLaggedOutput)\n",
    "                            + '_LossFunc:' + str(lossFunc)\n",
    "                            + '_OptimizerFunc:' + str(optimizerFunc))\n",
    "        \n",
    "        supervisedDataset = self.__timeSeriesToSupervised(lagCount, useLaggedOutput)\n",
    "        featureCount = len(self.__eegHeader) - 1\n",
    "        \n",
    "        splitPoint = int(testSetRatio * supervisedDataset.shape[0])\n",
    "        values = supervisedDataset.values\n",
    "        \n",
    "        trainingSet = values[:splitPoint, :]\n",
    "        testingSet = values[splitPoint:, :]\n",
    "        \n",
    "        ################################################\n",
    "        print 'featureCount * lagCount: ', featureCount * lagCount\n",
    "        pastInputCount = featureCount * lagCount\n",
    "        ################################################\n",
    "        \n",
    "        self.__trainTestMap = {}\n",
    "\n",
    "        self.__trainTestMap['trainingSet_inFeatures'] = trainingSet[:, :-1]\n",
    "        self.__trainTestMap['trainingSet_outFeature'] = trainingSet[:, -1]\n",
    "        \n",
    "        self.__trainTestMap['testingSet_inFeatures'] = testingSet[:, :-1]\n",
    "        self.__trainTestMap['testingSet_outFeature'] = testingSet[:, -1]\n",
    "        \n",
    "#         # Reshape input to be 3D [samples, timesteps, features]\n",
    "#         featureDimSize = (((featureCount + 1) * lagCount) + featureCount) / lagCount\n",
    "        \n",
    "#         self.__trainTestMap['trainingSet_inFeatures_reShaped'] = self.__trainTestMap['trainingSet_inFeatures'].reshape(\n",
    "#             (self.__trainTestMap['trainingSet_inFeatures'].shape[0], lagCount, featureDimSize))\n",
    "        \n",
    "#         print self.__trainTestMap['trainingSet_inFeatures_reShaped'].shape \n",
    "        \n",
    "#         self.__trainTestMap['testingSet_inFeatures_reShaped'] = self.__trainTestMap['testingSet_inFeatures'].reshape(\n",
    "#             (self.__trainTestMap['testingSet_inFeatures'].shape[0], lagCount, featureDimSize))\n",
    "        \n",
    "#         # Design network\n",
    "#         self.__model = Sequential()\n",
    "        \n",
    "#         self.__model.add(\n",
    "#             LSTM(neuronCount,\n",
    "#                  input_shape=(\n",
    "#                      self.__trainTestMap['trainingSet_inFeatures_reShaped'].shape[1],\n",
    "#                      self.__trainTestMap['trainingSet_inFeatures_reShaped'].shape[2])\n",
    "#                 )\n",
    "#         )\n",
    "        \n",
    "#         self.__model.add(Dense(1))\n",
    "#         self.__model.compile(loss=lossFunc, optimizer=optimizerFunc)\n",
    "        \n",
    "#         print self.__model.summary()\n",
    "        \n",
    "#         # Visualize LSTM network\n",
    "#         pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + modelArchitectureFilename\n",
    "#         plot_model(self.__model, to_file=pathToFile, show_shapes=True)\n",
    "        \n",
    "    def FitModel(self, epochCount, batchSize,\n",
    "                 resultsFile,\n",
    "                 lossFuncFilePrefix=None,\n",
    "                 predictionFilePrefix=None,\n",
    "                 figWidth=20, figHeight=20, verbosity=2):\n",
    "        assert self.__model != None, 'Model not compiled'\n",
    "        \n",
    "        self.__addToHistory('FitModel_EpochCount:' + str(epochCount)\n",
    "                            + '_BatchSize:' + str(batchSize)\n",
    "                            + '_lossFuncFilePrefix:' + str(lossFuncFilePrefix)\n",
    "                            + '_predictionFilePrefix:' + str(predictionFilePrefix))\n",
    "        \n",
    "        # Fit model\n",
    "        history = self.__model.fit(self.__trainTestMap['trainingSet_inFeatures_reShaped'],\n",
    "                                 self.__trainTestMap['trainingSet_outFeature'],\n",
    "                                 epochs=epochCount,\n",
    "                                 batch_size=batchSize,\n",
    "                                 validation_data=(\n",
    "                                     self.__trainTestMap['testingSet_inFeatures_reShaped'],\n",
    "                                     self.__trainTestMap['testingSet_outFeature']\n",
    "                                 ),\n",
    "                                 verbose=verbosity, shuffle=False)\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.grid()\n",
    "        ax.set_title('Loss Function')\n",
    "        ax.plot(history.history['loss'], label='train')\n",
    "        ax.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend(['train', 'test'])\n",
    "                    \n",
    "        if lossFuncFilePrefix is not None:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + lossFuncFilePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "\n",
    "        testDataPrediction = self.__model.predict_classes(self.__trainTestMap['testingSet_inFeatures_reShaped'])\n",
    "        testDataPredictionRaw = self.__model.predict(self.__trainTestMap['testingSet_inFeatures_reShaped'])\n",
    "                \n",
    "        scoringMetrics = Scoring_Functions(testDataPrediction, self.__trainTestMap['testingSet_outFeature'])\n",
    "        self.__writeResultsToFile(resultsFile, scoringMetrics.scoring_results())\n",
    "        print 'Results:\\n' + scoringMetrics.scoring_results()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.set_figwidth(figWidth)\n",
    "        fig.set_figheight(figHeight)\n",
    "        \n",
    "        ax1 = fig.add_subplot(3, 1, 1)\n",
    "        ax1.grid()\n",
    "        ax1.set_title('Expected')\n",
    "        ax1.plot(self.__trainTestMap['testingSet_outFeature'])\n",
    "\n",
    "        ax2 = fig.add_subplot(3, 1, 2)\n",
    "        ax2.grid()\n",
    "        ax2.set_title('Prediction - With rounding')\n",
    "        ax2.plot(testDataPrediction)\n",
    "\n",
    "        ax3 = fig.add_subplot(3, 1, 3)\n",
    "        ax3.grid()\n",
    "        ax3.set_title('Prediction - Without rounding')\n",
    "        ax3.plot(testDataPredictionRaw)\n",
    "\n",
    "        if predictionFilePrefix is not None:\n",
    "            pathToFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + predictionFilePrefix\n",
    "            plt.savefig(pathToFile)\n",
    "\n",
    "        # Reset __model\n",
    "        self.__model = None\n",
    "        self.__trainTestMap = None\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Private methods\n",
    "    def __createTestResultsDir(self):\n",
    "        dirName = \"%s/%s\" % (self.__resultsRootDir, self.__testID)\n",
    "        if os.path.exists(dirName):\n",
    "            shutil.rmtree(dirName)\n",
    "        os.makedirs(dirName)\n",
    "        \n",
    "    def __loadData(self, filename):\n",
    "        self.__eegData = np.genfromtxt(filename, delimiter=',', skip_header=1)\n",
    "        self.__eegHeader = np.genfromtxt(filename, delimiter=',', max_rows=1, dtype=str)\n",
    "\n",
    "    def __timeSeriesToSupervised(self, lagCount, useLaggedOutput):\n",
    "        featureCount = len(self.__eegHeader) - 1\n",
    "\n",
    "        cols, names = list(), list()\n",
    "        dfInput = pd.DataFrame(self.__eegData[:, :-1])\n",
    "        dfOutput = pd.DataFrame(self.__eegData[:, -1])\n",
    "        \n",
    "        for i in range(lagCount, 0, -1):\n",
    "            cols.append(dfInput.shift(i))\n",
    "            if useLaggedOutput:\n",
    "                cols.append(dfOutput.shift(i))\n",
    "            else:\n",
    "                # Set the lagged output variable to a constant 0.5\n",
    "                tmp = len(dfOutput.shift(i))\n",
    "                cols.append(pd.DataFrame(np.ones(tmp) * 0.5))\n",
    "            \n",
    "            names += [('%s(t-%d)' % (self.__eegHeader[j], i)) for j in range(featureCount)]\n",
    "            names += [('%s(t-%d)' % (self.__eegHeader[-1], i))]\n",
    "\n",
    "        cols.append(dfInput)\n",
    "        cols.append(dfOutput)\n",
    "        names += [('%s(t)' % self.__eegHeader[j]) for j in range(featureCount)]\n",
    "        names += [('%s(t)' % self.__eegHeader[-1])]\n",
    "\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "\n",
    "        # Drop rows containing NaN values\n",
    "        agg.dropna(inplace=True)\n",
    "        return agg\n",
    "    \n",
    "    def __addToHistory(self, newCmd):\n",
    "        tmpStr = ''\n",
    "        if self.__cmdCount > 0:\n",
    "            tmpStr += '\\n'\n",
    "            for itr in range(self.__cmdCount):\n",
    "                tmpStr += '+'\n",
    "                \n",
    "        tmpStr += newCmd\n",
    "        self.__cmdHistory += tmpStr\n",
    "        self.__cmdCount += 1\n",
    "        \n",
    "    def __writeResultsToFile(self, resFilename, results):\n",
    "        pathToResultsFile = self.__resultsRootDir + \"/\" + self.__testID + \"/\" + resFilename\n",
    "        resString = ''\n",
    "        \n",
    "        if os.path.exists(pathToResultsFile):\n",
    "            resString = '\\n\\n-----------------------------------------------------------------\\n'\n",
    "            fileMode = 'a' # Append\n",
    "        else:\n",
    "            fileMode = 'w' # Create\n",
    "        \n",
    "        text_file = open(pathToResultsFile, fileMode)\n",
    "        \n",
    "        resString += 'Results:\\n'\n",
    "        resString += results\n",
    "        resString += '\\n\\nCommand history:\\n----------------\\n'\n",
    "        resString += self.__cmdHistory        \n",
    "\n",
    "        text_file.write(resString)\n",
    "        text_file.close()\n",
    "    ###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestID:  9\n",
      "featureCount * lagCount:  28\n",
      "(2994, 2, 22)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 50)                14600     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 14,651\n",
      "Trainable params: 14,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "-----------------------\n",
      "RemoveOutliers_Upper:5000_Lower:None\n",
      "+NormalizeData_Func:MinMax\n",
      "++CompileModel_Lag:2_TestSize:0.2_NeuronCount:50_DropOut:0.0_useLaggedOutput:True_LossFunc:mae_OptimizerFunc:adam\n"
     ]
    }
   ],
   "source": [
    "testID += 1\n",
    "\n",
    "resultsFilename = 'TestResults.txt'\n",
    "print \"TestID: \", testID\n",
    "a = LSTMPredictor(testID, 'EEGEyeState.arff.csv')\n",
    "\n",
    "\n",
    "res = a.RemoveOutliers(upperLimit=5000)\n",
    "a.NormalizeData()\n",
    "# a.PlotFeatures(filePrefix='01_RawData')\n",
    "\n",
    "lagValue = 2\n",
    "\n",
    "a.CompileModel(lagCount=lagValue\n",
    "               , testSetRatio=0.2\n",
    "               , neuronCount=50\n",
    "               , dropOut=0.0\n",
    "               , useLaggedOutput=True\n",
    "               , modelArchitectureFilename='01_modelArchitecture')\n",
    "\n",
    "# a.FitModel(epochCount=10\n",
    "#            , batchSize=50\n",
    "#            , resultsFile=resultsFilename\n",
    "#            , lossFuncFilePrefix='01_lossFile'\n",
    "#            , predictionFilePrefix='01_predictionFile'\n",
    "#            , verbosity=0)\n",
    "\n",
    "\n",
    "print '\\n\\n-----------------------\\n' + a.GetCmdHistory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "100\n",
      "(100, 3)\n",
      "100\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 1, 3)              84        \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 1, 3)              84        \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 1, 3)              84        \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 1, 3)              84        \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 1, 3)              84        \n",
      "=================================================================\n",
      "Total params: 420\n",
      "Trainable params: 420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      " - 2s - loss: 52.5000 - val_loss: 52.4996\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 52.4995 - val_loss: 52.4990\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 52.4989 - val_loss: 52.4985\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 52.4983 - val_loss: 52.4979\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 52.4978 - val_loss: 52.4973\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 52.4972 - val_loss: 52.4968\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 52.4966 - val_loss: 52.4962\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 52.4960 - val_loss: 52.4956\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 52.4955 - val_loss: 52.4950\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 52.4949 - val_loss: 52.4944\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 52.4943 - val_loss: 52.4938\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 52.4937 - val_loss: 52.4932\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 52.4931 - val_loss: 52.4927\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 52.4925 - val_loss: 52.4920\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 52.4919 - val_loss: 52.4914\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 52.4913 - val_loss: 52.4908\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 52.4906 - val_loss: 52.4902\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 52.4900 - val_loss: 52.4895\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 52.4894 - val_loss: 52.4889\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 52.4887 - val_loss: 52.4882\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 52.4881 - val_loss: 52.4876\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 52.4874 - val_loss: 52.4869\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 52.4867 - val_loss: 52.4862\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 52.4860 - val_loss: 52.4855\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 52.4854 - val_loss: 52.4848\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 52.4847 - val_loss: 52.4841\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 52.4839 - val_loss: 52.4834\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 52.4832 - val_loss: 52.4827\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 52.4825 - val_loss: 52.4819\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 52.4818 - val_loss: 52.4812\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 52.4810 - val_loss: 52.4804\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 52.4802 - val_loss: 52.4797\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 52.4795 - val_loss: 52.4789\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 52.4787 - val_loss: 52.4781\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 52.4779 - val_loss: 52.4773\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 52.4771 - val_loss: 52.4764\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 52.4762 - val_loss: 52.4756\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 52.4754 - val_loss: 52.4747\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 52.4745 - val_loss: 52.4739\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 52.4736 - val_loss: 52.4730\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 52.4727 - val_loss: 52.4721\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 52.4718 - val_loss: 52.4711\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 52.4709 - val_loss: 52.4702\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 52.4700 - val_loss: 52.4692\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 52.4690 - val_loss: 52.4683\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 52.4680 - val_loss: 52.4673\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 52.4670 - val_loss: 52.4663\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 52.4660 - val_loss: 52.4653\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 52.4650 - val_loss: 52.4642\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 52.4640 - val_loss: 52.4632\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 52.4629 - val_loss: 52.4621\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 52.4618 - val_loss: 52.4610\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 52.4607 - val_loss: 52.4599\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 52.4596 - val_loss: 52.4587\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 52.4584 - val_loss: 52.4576\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 52.4573 - val_loss: 52.4564\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 52.4561 - val_loss: 52.4551\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 52.4548 - val_loss: 52.4539\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 52.4536 - val_loss: 52.4526\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 52.4523 - val_loss: 52.4513\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 52.4510 - val_loss: 52.4500\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 52.4497 - val_loss: 52.4487\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 52.4483 - val_loss: 52.4473\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 52.4470 - val_loss: 52.4459\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 52.4456 - val_loss: 52.4445\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 52.4441 - val_loss: 52.4430\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 52.4427 - val_loss: 52.4415\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 52.4412 - val_loss: 52.4400\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 52.4396 - val_loss: 52.4385\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 52.4381 - val_loss: 52.4369\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 52.4365 - val_loss: 52.4352\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 52.4348 - val_loss: 52.4336\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 52.4332 - val_loss: 52.4319\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 52.4315 - val_loss: 52.4302\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 52.4297 - val_loss: 52.4284\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 52.4279 - val_loss: 52.4266\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 52.4261 - val_loss: 52.4247\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 52.4243 - val_loss: 52.4229\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 52.4224 - val_loss: 52.4209\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 52.4204 - val_loss: 52.4189\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 52.4184 - val_loss: 52.4169\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 52.4164 - val_loss: 52.4149\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 52.4143 - val_loss: 52.4128\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 52.4122 - val_loss: 52.4106\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 52.4100 - val_loss: 52.4084\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 52.4078 - val_loss: 52.4061\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 52.4056 - val_loss: 52.4038\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 52.4032 - val_loss: 52.4015\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 52.4009 - val_loss: 52.3991\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 52.3984 - val_loss: 52.3966\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 52.3960 - val_loss: 52.3941\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 52.3934 - val_loss: 52.3915\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 52.3908 - val_loss: 52.3889\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 52.3882 - val_loss: 52.3862\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 52.3855 - val_loss: 52.3834\n",
      "Epoch 96/1000\n",
      " - 0s - loss: 52.3827 - val_loss: 52.3806\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 52.3799 - val_loss: 52.3777\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 52.3770 - val_loss: 52.3748\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 52.3741 - val_loss: 52.3718\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 52.3711 - val_loss: 52.3688\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 52.3680 - val_loss: 52.3657\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 52.3649 - val_loss: 52.3625\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 52.3617 - val_loss: 52.3593\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 52.3584 - val_loss: 52.3560\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 52.3551 - val_loss: 52.3526\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 52.3517 - val_loss: 52.3492\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 52.3483 - val_loss: 52.3457\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 52.3448 - val_loss: 52.3421\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 52.3412 - val_loss: 52.3385\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 52.3376 - val_loss: 52.3348\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 52.3339 - val_loss: 52.3311\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 52.3302 - val_loss: 52.3273\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 52.3263 - val_loss: 52.3234\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 52.3225 - val_loss: 52.3195\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 52.3185 - val_loss: 52.3155\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 52.3145 - val_loss: 52.3115\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 52.3105 - val_loss: 52.3074\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 52.3063 - val_loss: 52.3032\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 52.3022 - val_loss: 52.2990\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 52.2979 - val_loss: 52.2947\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 52.2937 - val_loss: 52.2904\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 52.2893 - val_loss: 52.2860\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 52.2849 - val_loss: 52.2816\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 52.2805 - val_loss: 52.2771\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 52.2760 - val_loss: 52.2726\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 52.2714 - val_loss: 52.2680\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 52.2668 - val_loss: 52.2633\n",
      "Epoch 128/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 52.2622 - val_loss: 52.2586\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 52.2575 - val_loss: 52.2539\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 52.2527 - val_loss: 52.2491\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 52.2479 - val_loss: 52.2443\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 52.2431 - val_loss: 52.2394\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 52.2382 - val_loss: 52.2345\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 52.2332 - val_loss: 52.2295\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 52.2282 - val_loss: 52.2243\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 52.2230 - val_loss: 52.2188\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 52.2174 - val_loss: 52.2133\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 52.2119 - val_loss: 52.2079\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 52.2065 - val_loss: 52.2025\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 52.2012 - val_loss: 52.1971\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 52.1957 - val_loss: 52.1916\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 52.1902 - val_loss: 52.1859\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 52.1844 - val_loss: 52.1798\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 52.1780 - val_loss: 52.1725\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 52.1705 - val_loss: 52.1647\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 52.1627 - val_loss: 52.1566\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 52.1547 - val_loss: 52.1490\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 52.1473 - val_loss: 52.1424\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 52.1408 - val_loss: 52.1361\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 52.1346 - val_loss: 52.1300\n",
      "Epoch 151/1000\n",
      " - 0s - loss: 52.1284 - val_loss: 52.1238\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 52.1223 - val_loss: 52.1177\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 52.1162 - val_loss: 52.1116\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 52.1101 - val_loss: 52.1055\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 52.1040 - val_loss: 52.0994\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 52.0979 - val_loss: 52.0933\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 52.0918 - val_loss: 52.0872\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 52.0856 - val_loss: 52.0810\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 52.0795 - val_loss: 52.0749\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 52.0734 - val_loss: 52.0687\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 52.0672 - val_loss: 52.0626\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 52.0610 - val_loss: 52.0564\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 52.0548 - val_loss: 52.0502\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 52.0486 - val_loss: 52.0440\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 52.0424 - val_loss: 52.0378\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 52.0362 - val_loss: 52.0315\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 52.0300 - val_loss: 52.0253\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 52.0237 - val_loss: 52.0190\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 52.0174 - val_loss: 52.0127\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 52.0111 - val_loss: 52.0064\n",
      "Epoch 171/1000\n",
      " - 0s - loss: 52.0048 - val_loss: 52.0001\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 51.9985 - val_loss: 51.9937\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 51.9921 - val_loss: 51.9874\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 51.9858 - val_loss: 51.9810\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 51.9794 - val_loss: 51.9746\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 51.9730 - val_loss: 51.9682\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 51.9666 - val_loss: 51.9617\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 51.9601 - val_loss: 51.9552\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 51.9536 - val_loss: 51.9488\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 51.9471 - val_loss: 51.9422\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 51.9406 - val_loss: 51.9357\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 51.9341 - val_loss: 51.9291\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 51.9275 - val_loss: 51.9225\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 51.9209 - val_loss: 51.9159\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 51.9142 - val_loss: 51.9092\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 51.9076 - val_loss: 51.9025\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 51.9009 - val_loss: 51.8959\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 51.8945 - val_loss: 51.8903\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 51.8889 - val_loss: 51.8849\n",
      "Epoch 190/1000\n",
      " - 0s - loss: 51.8835 - val_loss: 51.8795\n",
      "Epoch 191/1000\n",
      " - 0s - loss: 51.8782 - val_loss: 51.8743\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 51.8730 - val_loss: 51.8691\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 51.8678 - val_loss: 51.8645\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 51.8635 - val_loss: 51.8605\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 51.8596 - val_loss: 51.8567\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 51.8557 - val_loss: 51.8529\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 51.8519 - val_loss: 51.8492\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 51.8482 - val_loss: 51.8455\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 51.8446 - val_loss: 51.8419\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 51.8410 - val_loss: 51.8383\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 51.8374 - val_loss: 51.8348\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 51.8340 - val_loss: 51.8319\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 51.8312 - val_loss: 51.8292\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 51.8285 - val_loss: 51.8265\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 51.8258 - val_loss: 51.8239\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 51.8232 - val_loss: 51.8213\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 51.8207 - val_loss: 51.8189\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 51.8183 - val_loss: 51.8165\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 51.8160 - val_loss: 51.8142\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 51.8136 - val_loss: 51.8119\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 51.8113 - val_loss: 51.8096\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 51.8091 - val_loss: 51.8074\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 51.8068 - val_loss: 51.8052\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 51.8046 - val_loss: 51.8030\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 51.8025 - val_loss: 51.8010\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 51.8005 - val_loss: 51.7991\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 51.7986 - val_loss: 51.7972\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 51.7967 - val_loss: 51.7953\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 51.7948 - val_loss: 51.7935\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 51.7931 - val_loss: 51.7919\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 51.7914 - val_loss: 51.7902\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 51.7898 - val_loss: 51.7886\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 51.7882 - val_loss: 51.7870\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 51.7866 - val_loss: 51.7855\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 51.7851 - val_loss: 51.7841\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 51.7838 - val_loss: 51.7832\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 51.7830 - val_loss: 51.7823\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 51.7821 - val_loss: 51.7815\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 51.7813 - val_loss: 51.7807\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 51.7805 - val_loss: 51.7799\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 51.7797 - val_loss: 51.7791\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 51.7789 - val_loss: 51.7783\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 51.7781 - val_loss: 51.7776\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 51.7774 - val_loss: 51.7768\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 51.7766 - val_loss: 51.7761\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 51.7759 - val_loss: 51.7753\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 51.7751 - val_loss: 51.7746\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 51.7744 - val_loss: 51.7739\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 51.7737 - val_loss: 51.7731\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 51.7730 - val_loss: 51.7724\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 51.7722 - val_loss: 51.7717\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 51.7715 - val_loss: 51.7710\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 51.7708 - val_loss: 51.7703\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 51.7701 - val_loss: 51.7696\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 51.7695 - val_loss: 51.7689\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 51.7688 - val_loss: 51.7683\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 51.7681 - val_loss: 51.7676\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 51.7674 - val_loss: 51.7669\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 51.7668 - val_loss: 51.7663\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 51.7661 - val_loss: 51.7656\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 51.7654 - val_loss: 51.7650\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 51.7648 - val_loss: 51.7643\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 51.7641 - val_loss: 51.7637\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 51.7635 - val_loss: 51.7630\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 51.7629 - val_loss: 51.7624\n",
      "Epoch 256/1000\n",
      " - 0s - loss: 51.7622 - val_loss: 51.7618\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 51.7616 - val_loss: 51.7611\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 51.7610 - val_loss: 51.7605\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 51.7604 - val_loss: 51.7602\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 51.7601 - val_loss: 51.7598\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 51.7597 - val_loss: 51.7595\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 51.7594 - val_loss: 51.7591\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 51.7590 - val_loss: 51.7588\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 51.7587 - val_loss: 51.7584\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 51.7583 - val_loss: 51.7581\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 51.7580 - val_loss: 51.7578\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 51.7577 - val_loss: 51.7574\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 51.7574 - val_loss: 51.7571\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 51.7570 - val_loss: 51.7568\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 51.7567 - val_loss: 51.7565\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 51.7564 - val_loss: 51.7561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272/1000\n",
      " - 0s - loss: 51.7561 - val_loss: 51.7558\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 51.7557 - val_loss: 51.7555\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 51.7554 - val_loss: 51.7552\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 51.7551 - val_loss: 51.7549\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 51.7548 - val_loss: 51.7546\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 51.7545 - val_loss: 51.7543\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 51.7542 - val_loss: 51.7540\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 51.7539 - val_loss: 51.7536\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 51.7536 - val_loss: 51.7534\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 51.7533 - val_loss: 51.7530\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 51.7530 - val_loss: 51.7528\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 51.7527 - val_loss: 51.7525\n",
      "Epoch 284/1000\n",
      " - 0s - loss: 51.7524 - val_loss: 51.7522\n",
      "Epoch 285/1000\n",
      " - 0s - loss: 51.7521 - val_loss: 51.7519\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 51.7518 - val_loss: 51.7516\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 51.7515 - val_loss: 51.7513\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 51.7512 - val_loss: 51.7510\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 51.7509 - val_loss: 51.7507\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 51.7506 - val_loss: 51.7504\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 51.7504 - val_loss: 51.7501\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 51.7501 - val_loss: 51.7499\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 51.7498 - val_loss: 51.7496\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 51.7495 - val_loss: 51.7493\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 51.7492 - val_loss: 51.7491\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 51.7491 - val_loss: 51.7490\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 51.7490 - val_loss: 51.7490\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 51.7490 - val_loss: 51.7489\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 51.7489 - val_loss: 51.7489\n",
      "Epoch 300/1000\n",
      " - 0s - loss: 51.7489 - val_loss: 51.7488\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 51.7488 - val_loss: 51.7488\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 51.7488 - val_loss: 51.7487\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 51.7487 - val_loss: 51.7487\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 51.7487 - val_loss: 51.7486\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 51.7486 - val_loss: 51.7486\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 51.7486 - val_loss: 51.7485\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 51.7485 - val_loss: 51.7485\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 51.7485 - val_loss: 51.7484\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 51.7484 - val_loss: 51.7484\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 51.7484 - val_loss: 51.7483\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 51.7483 - val_loss: 51.7483\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 51.7483 - val_loss: 51.7482\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 51.7482 - val_loss: 51.7482\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 51.7482 - val_loss: 51.7482\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 51.7481 - val_loss: 51.7481\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 51.7481 - val_loss: 51.7481\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 51.7481 - val_loss: 51.7480\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 51.7480 - val_loss: 51.7480\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 51.7480 - val_loss: 51.7479\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 51.7479 - val_loss: 51.7479\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 51.7479 - val_loss: 51.7478\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 51.7478 - val_loss: 51.7478\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 51.7478 - val_loss: 51.7478\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 51.7478 - val_loss: 51.7477\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 51.7477 - val_loss: 51.7477\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 51.7477 - val_loss: 51.7476\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 51.7476 - val_loss: 51.7476\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 51.7476 - val_loss: 51.7476\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 51.7476 - val_loss: 51.7475\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 51.7475 - val_loss: 51.7475\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 51.7475 - val_loss: 51.7475\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 51.7474 - val_loss: 51.7474\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 51.7474 - val_loss: 51.7474\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 51.7474 - val_loss: 51.7473\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 51.7473 - val_loss: 51.7473\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 51.7473 - val_loss: 51.7473\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 51.7472 - val_loss: 51.7472\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 51.7472 - val_loss: 51.7472\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 51.7472 - val_loss: 51.7471\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 51.7471 - val_loss: 51.7471\n",
      "Epoch 341/1000\n",
      " - 0s - loss: 51.7471 - val_loss: 51.7471\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 51.7471 - val_loss: 51.7470\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 51.7470 - val_loss: 51.7470\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 51.7470 - val_loss: 51.7470\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 51.7470 - val_loss: 51.7469\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 51.7469 - val_loss: 51.7469\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 51.7469 - val_loss: 51.7469\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 51.7468 - val_loss: 51.7468\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 51.7468 - val_loss: 51.7468\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 51.7468 - val_loss: 51.7467\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 51.7467 - val_loss: 51.7467\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 51.7467 - val_loss: 51.7467\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 51.7467 - val_loss: 51.7467\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 51.7466 - val_loss: 51.7466\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 51.7466 - val_loss: 51.7466\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 51.7466 - val_loss: 51.7465\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 51.7465 - val_loss: 51.7465\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 51.7465 - val_loss: 51.7465\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 51.7465 - val_loss: 51.7464\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 51.7464 - val_loss: 51.7464\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 51.7464 - val_loss: 51.7464\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 51.7464 - val_loss: 51.7464\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 51.7463 - val_loss: 51.7463\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 51.7463 - val_loss: 51.7463\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 51.7463 - val_loss: 51.7463\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 51.7462 - val_loss: 51.7462\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 51.7462 - val_loss: 51.7462\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 51.7462 - val_loss: 51.7462\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 51.7462 - val_loss: 51.7461\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 51.7461 - val_loss: 51.7461\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 51.7461 - val_loss: 51.7461\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 51.7461 - val_loss: 51.7460\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 51.7460 - val_loss: 51.7460\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 51.7460 - val_loss: 51.7460\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 51.7460 - val_loss: 51.7459\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 51.7459 - val_loss: 51.7459\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 51.7459 - val_loss: 51.7459\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 51.7459 - val_loss: 51.7459\n",
      "Epoch 379/1000\n",
      " - 0s - loss: 51.7459 - val_loss: 51.7458\n",
      "Epoch 380/1000\n",
      " - 0s - loss: 51.7458 - val_loss: 51.7458\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 51.7458 - val_loss: 51.7458\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 51.7458 - val_loss: 51.7457\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 51.7457 - val_loss: 51.7457\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 51.7457 - val_loss: 51.7457\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 51.7457 - val_loss: 51.7457\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 51.7457 - val_loss: 51.7456\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 51.7456 - val_loss: 51.7456\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 51.7456 - val_loss: 51.7456\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 51.7456 - val_loss: 51.7456\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 51.7456 - val_loss: 51.7455\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 51.7455 - val_loss: 51.7455\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 51.7455 - val_loss: 51.7455\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 51.7455 - val_loss: 51.7454\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 51.7454 - val_loss: 51.7454\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 51.7454 - val_loss: 51.7454\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 51.7454 - val_loss: 51.7454\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 51.7454 - val_loss: 51.7453\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 51.7453 - val_loss: 51.7453\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 51.7453 - val_loss: 51.7453\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 51.7453 - val_loss: 51.7453\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 51.7453 - val_loss: 51.7452\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 51.7452 - val_loss: 51.7452\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 51.7452 - val_loss: 51.7452\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 51.7452 - val_loss: 51.7452\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 51.7452 - val_loss: 51.7451\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 51.7451 - val_loss: 51.7451\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 51.7451 - val_loss: 51.7451\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 51.7451 - val_loss: 51.7451\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 51.7451 - val_loss: 51.7450\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 51.7450 - val_loss: 51.7450\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 51.7450 - val_loss: 51.7450\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 51.7450 - val_loss: 51.7450\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 51.7450 - val_loss: 51.7449\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 51.7449 - val_loss: 51.7449\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 51.7449 - val_loss: 51.7449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416/1000\n",
      " - 0s - loss: 51.7449 - val_loss: 51.7449\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 51.7449 - val_loss: 51.7449\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 51.7449 - val_loss: 51.7448\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 51.7448 - val_loss: 51.7448\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 51.7448 - val_loss: 51.7448\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 51.7448 - val_loss: 51.7448\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 51.7448 - val_loss: 51.7447\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 51.7447 - val_loss: 51.7447\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 51.7447 - val_loss: 51.7447\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 51.7447 - val_loss: 51.7447\n",
      "Epoch 426/1000\n",
      " - 0s - loss: 51.7447 - val_loss: 51.7447\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 51.7446 - val_loss: 51.7446\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 51.7446 - val_loss: 51.7446\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 51.7446 - val_loss: 51.7446\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 51.7446 - val_loss: 51.7446\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 51.7446 - val_loss: 51.7445\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 51.7445 - val_loss: 51.7445\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 51.7445 - val_loss: 51.7445\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 51.7445 - val_loss: 51.7445\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 51.7445 - val_loss: 51.7445\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 51.7445 - val_loss: 51.7444\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 51.7444 - val_loss: 51.7444\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 51.7444 - val_loss: 51.7444\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 51.7444 - val_loss: 51.7444\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 51.7444 - val_loss: 51.7444\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 51.7444 - val_loss: 51.7443\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 51.7443 - val_loss: 51.7443\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 51.7443 - val_loss: 51.7443\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 51.7443 - val_loss: 51.7443\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 51.7443 - val_loss: 51.7443\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 51.7443 - val_loss: 51.7442\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 51.7442 - val_loss: 51.7442\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 51.7442 - val_loss: 51.7442\n",
      "Epoch 449/1000\n",
      " - 0s - loss: 51.7442 - val_loss: 51.7442\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 51.7442 - val_loss: 51.7442\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 51.7442 - val_loss: 51.7441\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 51.7441 - val_loss: 51.7441\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 51.7441 - val_loss: 51.7441\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 51.7441 - val_loss: 51.7441\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 51.7441 - val_loss: 51.7441\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 51.7441 - val_loss: 51.7440\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 51.7440 - val_loss: 51.7440\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 51.7440 - val_loss: 51.7440\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 51.7440 - val_loss: 51.7440\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 51.7440 - val_loss: 51.7440\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 51.7440 - val_loss: 51.7440\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 51.7439 - val_loss: 51.7439\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 51.7439 - val_loss: 51.7439\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 51.7439 - val_loss: 51.7439\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 51.7439 - val_loss: 51.7439\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 51.7439 - val_loss: 51.7439\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 51.7439 - val_loss: 51.7438\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 51.7438 - val_loss: 51.7438\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 51.7438 - val_loss: 51.7438\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 51.7438 - val_loss: 51.7438\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 51.7438 - val_loss: 51.7438\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 51.7438 - val_loss: 51.7438\n",
      "Epoch 473/1000\n",
      " - 0s - loss: 51.7438 - val_loss: 51.7437\n",
      "Epoch 474/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7437\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7437\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7437\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7437\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7437\n",
      "Epoch 479/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7437\n",
      "Epoch 480/1000\n",
      " - 0s - loss: 51.7437 - val_loss: 51.7436\n",
      "Epoch 481/1000\n",
      " - 0s - loss: 51.7436 - val_loss: 51.7436\n",
      "Epoch 482/1000\n",
      " - 0s - loss: 51.7436 - val_loss: 51.7436\n",
      "Epoch 483/1000\n",
      " - 0s - loss: 51.7436 - val_loss: 51.7436\n",
      "Epoch 484/1000\n",
      " - 0s - loss: 51.7436 - val_loss: 51.7436\n",
      "Epoch 485/1000\n",
      " - 0s - loss: 51.7436 - val_loss: 51.7436\n",
      "Epoch 486/1000\n",
      " - 0s - loss: 51.7436 - val_loss: 51.7435\n",
      "Epoch 487/1000\n",
      " - 0s - loss: 51.7435 - val_loss: 51.7435\n",
      "Epoch 488/1000\n",
      " - 0s - loss: 51.7435 - val_loss: 51.7435\n",
      "Epoch 489/1000\n",
      " - 0s - loss: 51.7435 - val_loss: 51.7435\n",
      "Epoch 490/1000\n",
      " - 0s - loss: 51.7435 - val_loss: 51.7435\n",
      "Epoch 491/1000\n",
      " - 0s - loss: 51.7435 - val_loss: 51.7435\n",
      "Epoch 492/1000\n",
      " - 0s - loss: 51.7435 - val_loss: 51.7434\n",
      "Epoch 493/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 494/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 495/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 496/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 497/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 498/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 499/1000\n",
      " - 0s - loss: 51.7434 - val_loss: 51.7434\n",
      "Epoch 500/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7433\n",
      "Epoch 501/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7433\n",
      "Epoch 502/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7433\n",
      "Epoch 503/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7433\n",
      "Epoch 504/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7433\n",
      "Epoch 505/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7433\n",
      "Epoch 506/1000\n",
      " - 0s - loss: 51.7433 - val_loss: 51.7432\n",
      "Epoch 507/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 508/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 509/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 510/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 511/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 512/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 513/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7432\n",
      "Epoch 514/1000\n",
      " - 0s - loss: 51.7432 - val_loss: 51.7431\n",
      "Epoch 515/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7431\n",
      "Epoch 516/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7431\n",
      "Epoch 517/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7431\n",
      "Epoch 518/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7431\n",
      "Epoch 519/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7431\n",
      "Epoch 520/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7431\n",
      "Epoch 521/1000\n",
      " - 0s - loss: 51.7431 - val_loss: 51.7430\n",
      "Epoch 522/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 523/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 524/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 525/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 526/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 527/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 528/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7430\n",
      "Epoch 529/1000\n",
      " - 0s - loss: 51.7430 - val_loss: 51.7429\n",
      "Epoch 530/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 531/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 532/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 533/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 534/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 535/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 536/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7429\n",
      "Epoch 537/1000\n",
      " - 0s - loss: 51.7429 - val_loss: 51.7428\n",
      "Epoch 538/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 539/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 540/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 541/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 542/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 543/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 544/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7428\n",
      "Epoch 545/1000\n",
      " - 0s - loss: 51.7428 - val_loss: 51.7427\n",
      "Epoch 546/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 547/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 548/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 549/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 550/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 551/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 552/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 553/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7427\n",
      "Epoch 554/1000\n",
      " - 0s - loss: 51.7427 - val_loss: 51.7426\n",
      "Epoch 555/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 556/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 557/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 558/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 559/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 560/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 561/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 562/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7426\n",
      "Epoch 563/1000\n",
      " - 0s - loss: 51.7426 - val_loss: 51.7425\n",
      "Epoch 564/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 565/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 566/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 567/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 568/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 569/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 570/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7425\n",
      "Epoch 571/1000\n",
      " - 0s - loss: 51.7425 - val_loss: 51.7424\n",
      "Epoch 572/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 573/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 574/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 575/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 576/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 577/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 578/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 579/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 580/1000\n",
      " - 0s - loss: 51.7424 - val_loss: 51.7424\n",
      "Epoch 581/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 582/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 583/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 584/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 585/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 586/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 587/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 588/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 589/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 590/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7423\n",
      "Epoch 591/1000\n",
      " - 0s - loss: 51.7423 - val_loss: 51.7422\n",
      "Epoch 592/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 593/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 594/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 595/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 596/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 597/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 598/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 599/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 600/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7422\n",
      "Epoch 601/1000\n",
      " - 0s - loss: 51.7422 - val_loss: 51.7421\n",
      "Epoch 602/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 603/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 604/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 605/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 606/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 607/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 608/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 609/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 610/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 611/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7421\n",
      "Epoch 612/1000\n",
      " - 0s - loss: 51.7421 - val_loss: 51.7420\n",
      "Epoch 613/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 614/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 615/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 616/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 617/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 618/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 619/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 620/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 621/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 622/1000\n",
      " - 0s - loss: 51.7420 - val_loss: 51.7420\n",
      "Epoch 623/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 624/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 625/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 626/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 627/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 628/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 629/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 630/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 631/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 632/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 633/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7419\n",
      "Epoch 634/1000\n",
      " - 0s - loss: 51.7419 - val_loss: 51.7418\n",
      "Epoch 635/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 636/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 637/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 638/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 639/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 640/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 641/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 642/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 643/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 644/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 645/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 646/1000\n",
      " - 0s - loss: 51.7418 - val_loss: 51.7418\n",
      "Epoch 647/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 648/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 649/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 650/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 651/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 652/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 653/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 654/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 655/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 656/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 657/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 658/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7417\n",
      "Epoch 659/1000\n",
      " - 0s - loss: 51.7417 - val_loss: 51.7416\n",
      "Epoch 660/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 661/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 662/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 663/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 664/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 665/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 666/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 667/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 668/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 669/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 670/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 671/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 672/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7416\n",
      "Epoch 673/1000\n",
      " - 0s - loss: 51.7416 - val_loss: 51.7415\n",
      "Epoch 674/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 675/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 676/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 677/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 678/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 679/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 680/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 681/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 682/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 683/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 684/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 685/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 686/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7415\n",
      "Epoch 687/1000\n",
      " - 0s - loss: 51.7415 - val_loss: 51.7414\n",
      "Epoch 688/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 689/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 690/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 691/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 692/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 693/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 694/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 695/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 696/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 697/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 698/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 699/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7414\n",
      "Epoch 700/1000\n",
      " - 0s - loss: 51.7414 - val_loss: 51.7413\n",
      "Epoch 701/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 702/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 703/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 704/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 705/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 706/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 707/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 708/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 709/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 710/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 711/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 712/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 713/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 714/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 715/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7413\n",
      "Epoch 716/1000\n",
      " - 0s - loss: 51.7413 - val_loss: 51.7412\n",
      "Epoch 717/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 718/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 719/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 720/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 721/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 722/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 723/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 724/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 725/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 726/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 727/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 728/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 729/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 730/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 731/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7412\n",
      "Epoch 732/1000\n",
      " - 0s - loss: 51.7412 - val_loss: 51.7411\n",
      "Epoch 733/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 734/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 735/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 736/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 737/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 738/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 739/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 740/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 741/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 742/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 743/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 744/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 745/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 746/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 747/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7411\n",
      "Epoch 748/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7410\n",
      "Epoch 749/1000\n",
      " - 0s - loss: 51.7411 - val_loss: 51.7410\n",
      "Epoch 750/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 751/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 752/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 753/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 754/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 755/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 756/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 757/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 758/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 759/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 760/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 761/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 762/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 763/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 764/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 765/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 766/1000\n",
      " - 0s - loss: 51.7410 - val_loss: 51.7410\n",
      "Epoch 767/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7410\n",
      "Epoch 768/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 769/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 770/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 771/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 772/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 773/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 774/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 775/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 776/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 777/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 778/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 779/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 780/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 781/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 782/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 783/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 784/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7409\n",
      "Epoch 785/1000\n",
      " - 0s - loss: 51.7409 - val_loss: 51.7408\n",
      "Epoch 786/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 787/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 788/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 789/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 790/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 791/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 792/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 793/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 794/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 795/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 796/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 797/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 798/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 799/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 800/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 801/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 802/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 803/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 804/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7408\n",
      "Epoch 805/1000\n",
      " - 0s - loss: 51.7408 - val_loss: 51.7407\n",
      "Epoch 806/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 807/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 808/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 809/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 810/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 811/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 812/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 813/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 814/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 815/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 816/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 817/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 818/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 819/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 820/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 821/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 822/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 823/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 824/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 825/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 826/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 827/1000\n",
      " - 0s - loss: 51.7407 - val_loss: 51.7407\n",
      "Epoch 828/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 829/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 830/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 831/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 832/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 833/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 834/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 835/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 836/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 837/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 838/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 839/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 840/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 841/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 842/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 843/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 844/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 845/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 846/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 847/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 848/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 849/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7406\n",
      "Epoch 850/1000\n",
      " - 0s - loss: 51.7406 - val_loss: 51.7405\n",
      "Epoch 851/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 852/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 853/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 854/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 855/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 856/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 857/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 858/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 859/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 860/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 861/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 862/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 863/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 864/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 865/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 866/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 867/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 868/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 869/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 870/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 871/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 872/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 873/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 874/1000\n",
      " - 0s - loss: 51.7405 - val_loss: 51.7405\n",
      "Epoch 875/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 876/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 877/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 878/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 879/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 880/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 881/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 882/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 883/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 884/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 885/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 886/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 887/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 888/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 889/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 890/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 891/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 892/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 893/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 894/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 895/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 896/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 897/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 898/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 899/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7404\n",
      "Epoch 900/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7403\n",
      "Epoch 901/1000\n",
      " - 0s - loss: 51.7404 - val_loss: 51.7403\n",
      "Epoch 902/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 903/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 904/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 905/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 906/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 907/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 908/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 909/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 910/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 911/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 912/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 913/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 914/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 915/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 916/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 917/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 918/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 919/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 920/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 921/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 922/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 923/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 924/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 925/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 926/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7403\n",
      "Epoch 927/1000\n",
      " - 0s - loss: 51.7403 - val_loss: 51.7402\n",
      "Epoch 928/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 929/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 930/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 931/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 932/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 933/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 934/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 935/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 936/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 937/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 938/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 939/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 940/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 941/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 942/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 943/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 944/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 945/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 946/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 947/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 948/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 949/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 950/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 951/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 952/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 953/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 954/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 955/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 956/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 957/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7402\n",
      "Epoch 958/1000\n",
      " - 0s - loss: 51.7402 - val_loss: 51.7401\n",
      "Epoch 959/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 960/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 961/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 962/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 963/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 964/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 965/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 966/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 967/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 968/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 969/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 970/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 971/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 972/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 973/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 974/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 975/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 976/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 977/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 978/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 979/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 980/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 981/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 982/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 983/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 984/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 985/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 986/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 987/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 988/1000\n",
      " - 0s - loss: 51.7401 - val_loss: 51.7401\n",
      "Epoch 989/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7401\n",
      "Epoch 990/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7401\n",
      "Epoch 991/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 992/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 993/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 994/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 995/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 996/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 997/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 998/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 999/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n",
      "Epoch 1000/1000\n",
      " - 0s - loss: 51.7400 - val_loss: 51.7400\n"
     ]
    }
   ],
   "source": [
    "leng = 3\n",
    "data = [[i + j for j in range(leng)] for i in range(100)]\n",
    "data = np.array(data, dtype=float)\n",
    "print data.shape\n",
    "print len(data)\n",
    "\n",
    "target = [[i + j + 1 for j in range(leng)] for i in range(1, 101)]\n",
    "target = np.array(target, dtype=float)\n",
    "print target.shape\n",
    "print len(target)\n",
    "\n",
    "dataReShaped = data.reshape(len(data), 1, leng)\n",
    "targetReShaped = target.reshape(len(target), 1, leng)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(leng, input_shape=(1, leng), return_sequences=True))\n",
    "model.add(LSTM(leng, input_shape=(1, leng), return_sequences=True))\n",
    "model.add(LSTM(leng, input_shape=(1, leng), return_sequences=True))\n",
    "model.add(LSTM(leng, input_shape=(1, leng), return_sequences=True))\n",
    "model.add(LSTM(leng, input_shape=(1, leng), return_sequences=True))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "lossFunc='mae'\n",
    "optimizerFunc='adam'\n",
    "\n",
    "model.compile(loss=lossFunc, optimizer=optimizerFunc)\n",
    "\n",
    "hist = model.fit(dataReShaped, targetReShaped, epochs=1000, batch_size=50, verbose=2,\n",
    "                 validation_data=(dataReShaped, targetReShaped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a406a49d0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEr+plF/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a82a8d610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n38yszDj4zczKzP8HwQgjqer6IuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a43512210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print hist.history.keys()\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist.history['val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keraEnv",
   "language": "python",
   "name": "keraenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
